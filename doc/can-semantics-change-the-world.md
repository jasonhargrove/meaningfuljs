Can semantic technologies change the world (of computers)?
==========================================================

### [Machines vs. humans](#mvh)
### [Reshaping meaning representation](#markup)
### [Reshaping user interface](#ui)
### [Reshaping application/data interfaces](#api)
### [Reshaping the Future](#future)
---

Is it possible to know everything? The answer depends on adherence to consistency or completeness. In terms of consistency, that's quite possible humans will be able to understand if not all but the most of laws of (at least) observable Nature. Will it be enough? Any scientific law is a dependency of constituents, which is considered in isolation of other factors. Any real situation is a combination of numerous and unknown factors multiplied on a variety of different laws of Nature. Something can be described with scientific laws only if all assumptions satisfied or, in other words, other or unknown factors do not affect the situation. If we cannot identify other factors, then, theoretically, we can react to an input, which can be considered as a combination of these factors. But without knowing what exactly influences the situation, we won't be able to anticipate future problems. How can we be sure we know all consistency rules of the Nature or of the specific situation? In terms of completeness, it is definitely not possible to know all facts about the Universe in its entirety. It is quite old problem, which states to keep all facts about something you need to have storage with the same volume as this something. And even if we can zip information, considering it is created constantly, we need quite powerful algorithms and hardware to handle real-time archiving. Moreover, if we consider facts about the past and future too, then the problem of knowing everything is theoretically possible only if we would have access to any information from the entire Universe in the whole lifespan. That is definitely beyond human abilities in the near future.

All this seemingly philosophical and even transcendent problems directly relate to the future of computer industry. Neither Natural nor Artificial Intelligence won't be able to know and process all available information. When someone believes that in some "bright" future this will be made possible by AI and supersmart-hyperintelligent-oversophisticated devices-agents-services, then such person just surrender in thoughts to them. There will be plenty of possibilities for both humans and machines. All depends only on scope of consideration. Big Data, which wants to become Huge One, has a lot of directions for analysis, you may push algorithms in hundred directions but there are thousands more. Do you believe very-very smart cars, which would be 100% bullet-proof against regular situations in city traffic, will survive a flood, an earthquake, an asteroid impact, the Sun outburst? Certainly no. If we consider them as "totally" smart, it is just because we consider a narrow area of applicability and ignore the rest of reality.

But even so, computers can do many things better than humans. Will it hurt us? Only if we would allow them to have emotions, awareness, will. But do we really want the situation like "It is soooo boring to run office applications, better I will play chess with my netmate"? As soon as they will be tools, we will be fine. Does it hurt us that we have vehicles, planes, other machines and processes, which are much more powerful than a human can do ever? We just use them. We collaborate with them. To have the same for computers and to understand each other, Natural and Artificial Intelligence should communicate in the same language. Semantics of which should be both human- and machine-friendly. Semantics should be widespread. Semantics should be an explicit tool for users. Humans should be motivated to see semantics behind words and use it in communication with computers and between each other.
 
And that’s really different from what we have now with semantic technologies. 60 years was enough for many computer technologies to flourish but not for semantic networks. Why? Evidently, because they have some inherent shortcomings, which restrict them. Possibly, because they have a discrete form with fixed level of details. Why do 15 years were enough for hypertext and not for Semantic Web? Maybe because it did not overgrow "Web of data" and possibly would stay it forever? Actually, it is a highly formalized machine-friendly representation of regular and repetitive information (including URI identification, triples, ontologies that are very similar to software modeling languages, and querying language that is similar to SQL), which is inherent rather for algorithms than humans. Is semantics of hypertext meaningful enough? Barely, as so called "semantic tags" may hint some words are accentuated but does not help to understand why. Are tags under articles meaningful enough? They rather creates limited sets of focused topics but it is not clear how they relate to each other. Are results produced by search engines meaningful enough? To some degree yes, if your query returned appropriate results at the first page but usually a user needs to filter search outcome manually. Ask yourself do humans understand each other basing on a statistical guess of unclear kind of relevancy? To some degree yes but only to some degree. Are virtual assistants (like Siri) smart enough to deal with semantics? They desperately try but results mostly cover standard search queries and standard applications for a specific device. Natural language processing? Is knowledge of what is a noun/verb/subject/predicate enough to understand some sentence?

Imagine you try to understand what "xxarrgh" means in some cosmic language basing on facts: (a) "xxarrgh" is an instance of "xyzie", (b) "xxarrgh" has "length" and "bbarrgh" properties, it can be "moved" and "ccarrghed", (c) "xxarrgh" is a noun. You can link these facts in semantic network, you can build a vocabulary and ontology in Semantic Web, you can find "xxarrgh" with search query by relevancy, you can even talk about it with a virtual assistant (which will answer you that "xxarrgh is a xyzie, you know"), you can even parse "xxarrgh ccarrghed by ghrraxx" to subject/predicates/object. But none of these approaches has real understanding behind that. Though today computers work with namely with such sort of semantics.

What do we imply under understanding then? In what case you will be able to understand what "xxarrgh" means? Evidently, if the word (and meaning behind it) will be explained in terms, which you are familiar with, what they are similar to, what they consist of or included to, etc. "xxarrgh is a blue sprout of half-vegetable half-fruit" is an example of such explanation, where all words comes from English, "is" expresses similarity of "xxarrgh" to "sprout", "blue" and "of a half-vegetable half-fruit" express a characteristic and an inclusion to the whole correspondingly. Wait a minute, but this is exactly what applications operate with: identifiers, "is-a" and "has-a" relations. Exactly, but their meaning and usage is slightly different for humans and applications as they use different approaches. And, by the way, don’t forget we know what "blue sprout" is and can imagine what "half-vegetable half-fruit" can be. But machines cannot.

<a name="mvh"></a>Machines vs. humans
-------------------

Machines tend to use formalized and narrowly focused approach, whereas humans prefer flexible one. Machine-friendly identification usually has own rules of name encoding (which produces names like JupiterPlanet or jup_planet) and inclined to be strictly unique. Human-friendly identification is meaningful (for humans), flexible, and allows to compose a sequence of any length for describing meaning with any level of details. For example, "the third orange leaf of the pot, which stands on the second floor of the beautiful building in suburb of the capital of the country with the highest rate of scientific discoveries". Such an identifier is unique but unique enough: it does not specify which planet we imply, the highest rate at what period, etc. That is, with natural language you can give as many details as possible for you at the moment and specify more, if necessary.

Computer-oriented "is-a" relation usually used to build hierarchy-based classifications. However, hierarchy is an artificial and arbitrary construction, as we can find any number of similarities between things. Thus, "xxarrgh" may be classified as "greens/vegetables/xyzie/xxarrgh", "botany/plants/fruits/xyzie/xxarrgh", "seed care/germinating/xxarrgh", etc. Another shortcoming of computer-friendly classifications, you need to choose how different "levels" of classification relate to each other. For example, "a sprout is an instance of a plant" (as a sprout is only a kind of a plant) or "a plant is an instance of a sprout" (as a sprout is a base for a plant), or maybe "a sprout is a state (property?) of a plant", or "a sprout is a mapping (function?) of a seed to a plant". Usually, software architecture implies only one variant of correspondence. If "plant" is a type, then usually it is not used as an identifier. If "green" is a value, then it is not used as a type. But that's not how we use "is" relation in natural language: (a) it is simple as usually we use only two levels as in "xxarrgh is a xyzie" and such relations form not a hierarchy but rather a graph, (b) it is relaxed as we don't need to specify what kind of "is" we use, (c) it is flexible as we can settle "is" relation between arbitrary things, which may imply cross-type generalization ("a sprout is a small green volcano").

Machine-based "has-a" relation usually describes some information according with some fixed pattern of details. That is, if we defined a plant has branches and leafs, then such model won't allow to represent flowers or composition of organic matters (like carbohydrates, fats). If we defined color as a property of the entire plant, it may not allow to specify color for each part. Unlike "is" and "has" relations, space/time/cause/effect and others (for example, math ones) usually are not represented as separate entities in programming languages and expressed with functions/methods/rules or are implicit. However, natural language considers them separately (at least implicitly) as they are required to answer "when?", "where?", "why?", and "how?" questions.

Undefined relation can express relation of any type (is/has/etc). Modern search engines operate with namely such type of relation. But if for computers such vague typing is rather a shortcoming, for humans it is rather an advantage because it allows more relaxed approach, as they may not know and have no time to specify exact type of relations. A variation of undefined relation is a combination/complex of meanings, when they are united by criteria, which may be known. For example, "plane fly" phrase in natural language or "plane.fly()" statements in programming language. Here undefined relation expresses dualism of space-time and object-actions in it. But "plane flight" or "iron bird en route" express similar even though we don't use "noun-verb" or "subject-predicate" combinations. Generally speaking, it is hard to separate an object from an action: a plane is constantly doing something, something constantly occurs with it. Nouns and verbs are just conventions what we consider as a static and what as a dynamic part of an object-action. Moreover, nouns and verbs are rather abstractions and a sort of relations. "plane fly" includes not only movement of the very plane but also engine work and pilot activities, etc. These examples show noun/verbs/subject/predicate/object are not crucial for understanding. The more matter what specific identifiers are similar to. "fly" and "flight" are different parts of natural language but they both are equivalent to more or less similar meaning.

Will these relations be enough? To disambiguate meaning and comparing with plain text, yes. For example, when "New York buildings" is represented as just three words, its meaning is ambiguous. But we can separate identifiers and relations as "New York | has | buildings" ("buildings of New York") or "New | of | York | has | buildings" ("new buildings of York", where "new" is a property/quality/adjective of "York"). Then even if "York" can be ambiguous too but the standalone identifier is much less vague than the entire phrase. Take another example: "beautiful plane flies quickly with two hundred passengers through the sky from Vienna to Paris". (1) "plane" has "beautiful" and "two hundreds passengers" properties, (2) "plane flies" or "flight" combination has "quickly", "through the sky", "from Vienna", "to Paris" parameters/properties, (3) additionally we can specify "flight" has "Vienna" as a value of departure property and "Paris" as one of an arrival one. Is it enough? For the start, yes. For user-friendly semantics, yes. What we need now is to introduce semantics on wider scale and make it available for users.

Though machine-friendly and human-friendly semantics are rather conditional names for different approaches. Machine-friendly semantics is narrowly focused and highly optimized in specific domain area but humans are so too. The most of humans are specialized in specific activities whereas interdisciplinary approach is somewhat rare skill. Human-friendly semantics is flexible and open but machines as a complex of different applications or even a network of different services can be flexible and open too assuming data can be shared between applications/services. The problem is not one kind of semantics is worse, the problem is both approaches should be used.

As for now, human-friendly semantics is present only in the ambiguous form of raw natural language text, where it is hidden for algorithms though understood by humans. How does computer industry propose to solve this? Semantic Web and similar approaches state semantics is a task for intelligent agents. Virtual assistants and similar approaches state they retrieve semantics from natural language input and convert into output. Any current approach rules out humans from the equation. We are in a vicious circle: we use machine-friendly semantics, because human-friendly semantics is not considered, and it is not considered, because we can use only machine-friendly one. How we can break it?

<a name="markup"></a>Reshaping meaning representation
--------------------------------

Try "sprout of apple" or "apple sprout" query. Results are (a) applications with "sprout" word in a title, (b) apple seeding, (c) apple sprouts recipes, (d) recipes with some plant sprouts (the most queries are about this). So, a search engine found information, where "sprout" word has the highest relevancy (the most popular?), did not try to group them, and these two queries with the same meaning from the point of view of natural language gave different results ("macintosh sprout" results are even more intriguing). Our query was about sprouts of namely apple. Meaning of this query is clear to, hopefully, 99% people who understand English. Users cannot rely on guesses of algorithms, they should have means to make semantics explicit in queries and have explicit results to understand if an algorithm correctly get semantics of a query. Would it be difficult for users? They do understand simple "is" and "has" relations. They do use them with natural language and in computers (user interface, file system, etc). Finally, algorithms may just suggest variants of explicit semantics to be finalized by users.

How can semantics be represented for users? One example was search results as above. Another example is knowledge navigators and semantic browsers, which sometimes just dump piles of related information with weak ordering and prioritizing. Yet another example is Wikipedia, where information is ordered and prioritized but is a result of ad-hoc editing and represents a static meaning scope. We need combine both dynamic and ad-hoc approaches. Natural language identifiers are formed very flexibly and we cannot have a static scope or a wiki page for each one-time identifier like "the third orange leaf of the pot, which stands on the second floor of the beautiful building in suburb of the capital of the country with the highest rate of scientific discoveries". Though ad-hoc approach could be preferable when you work with limited domain area.

Then what? One possible solution is [meaningful markup](https://github.com/meaningfuljs/meaningfuljs). It may help to outline and disambiguate simplified semantics in a text. Such markup is similar to hypertext (or can be integrated into it) with important distinctions: (a) usage of curly (or square) brackets not angular ones, (b) usage of human-friendly semantics identifiers not Web resources, (c) usage of semantic relations (which may be present in text too) not tags. For example, "xxarrgh {is} a blue sprout {of} half-vegetable half-fruit". That is, we separated identifiers and linked them with "is" and "of" relations. Markup composing may be helped with user interface by suggesting disambiguating identifiers, borders between them, known relations, etc. Will users use such markup? Do they use advanced search syntax? Everything depends on motivation. The more chances for better results, the more users will use it.

Of course, we do not propose to markup big volumes of information. If algorithms were able to do this one day, good for them. And for us. But as for now, they can only try to guess, whereas humans can do this at least for small but important chunks of information. And, certainly, they can collaborate with each other. One possibility of collaboration is two-way conversation, where algorithm guesses can be corrected by humans. Another possibility is to help humans with markup of summarizing meaning only. Will it be enough? Summaries play one of the most important role in semantics. Search results are more reliable if they return documents with a query words in a title but not in a body. If a title adequately summarizes document content, a search has higher probability to find this document. Therefore, the situation can improve even if only summaries/titles will become meaningful.

But semantics can be used not only for marking up information but also for its organizing. There is an essential difference between search and navigation, or global search and local one. The former is about finding something in indefinite volume of information, the latter is about definite one. It would make no sense to search your documents in several thousands other files at hard drive. You know they are here, you know there are only several ones, you know they are the most important information for you, therefore navigation is the better and faster option here. And to make navigation possible you need a meaningful organizing of your documents, ordering and prioritizing. That is, ad-hoc information created by you should have priority over found with a search. Search results for entities with too wide scope (like "Universe" or "physics") should be restricted. "is" relation of generally accepted definition should have priority over rarer ones: "sprout is a shoot of a plant" should be preferred over imaginary "a sprout is a small green volcano". "has" relations of generally accepted properties should have priority over others. Such ordering and prioritizing is the essence of user interface, which helps to organize information in human-friendly way.

<a name="ui"></a>Reshaping user interface
------------------------

Any kind of user interface (GUI, command-line, web, etc) is still not ready for natural language and semantics. Voice commands try to handle an interface literally. But it is not very efficient because pronouncing commands can be even longer than clicking and you should know GUI paths how to reach a target control. Virtual assistants do this more efficiently but not efficiently enough. They try to re-interpret applications as rather "flat" destination, that is, to reach any control with simple natural language statements. But this is restricted only with standard applications. For third-party ones it is proposed to use either some API or standards, which are based rather on machine-friendly semantics. All this stipulates somewhat slow adoption of these technologies

Why do we need user interface at all? UI provides human-friendly representation (text, images, audio, video) of computer data for users. But even such representation implies humans have to extract meaning from it. Of course, it can be easy but it can be ambiguous, moreover, it can be quite complicated. Another function of user interface is providing ways to reach a target information or controls. And we still use machine-oriented way to do this, we still think in terms of files, applications, windows, widgets, etc, etc. On one hand, that's reasonable as when we work with some tool we use its traits and specifics. On another, that is unnecessary details, finally, when we use any tool we are driven by our intentions and when a tool is mastered we work with it intuitively without conscious consideration of its principles of work. It seems just a question of habit and experience. But what's really wrong here: computer tools make us to think and operate in the way, which is not quite efficient as for semantics. And, in part, it is the result of these tools use semantics, which is, in part, machine-friendly, sometimes implicit, sometimes duplicated, sometimes misused, and sometimes just not used.

For example, file and GUI paths (which consist of controls, clicks, key presses, etc) implies unique classification of things. That is, each file has a unique path of classification, each GUI element has a unique path of clicks-opens, which is a sort of classification too (for example, "click Account -> press Operation -> click Deposit"). Yes, there are symbolic links and often GUI controls can be reached in several ways but in any case we need to know these ways. Though classification usually implies "is" relation, file and GUI paths may use "has" and other relations too (for example, "SolarSystem/planets/Jupiter/chemistry/reports") and we have no means to specify relation types. In the case of file paths, we have another shortcoming: if we copy a file from one computer to another, usually we are to re-classify it when saving. One more flaw: identification is often duplicated in a document content and file name, which, in own turn, may be not meaningful (if a user deformed or abbreviated it by own rules). All this leads to troubles when a user searches/navigates for something because (a) numerous long paths are hard to remember, (b) classification information (which is defined by file or GUI path) is not used for searching, (c) classification may be not meaningful enough, (d) we cannot extract information from GUI with regular text search.

What will change, if human-friendly semantics be used in explicit and appropriate form?  Instead of long file paths, we can use human-friendly identification, which enables straightforward access to information. For example, instead of "Galaxy/SolarSystem/planets/Jupiter/chemistry/reports" path we can use more concise "report {of} chemistry {of} Jupiter" identifier as the rest of information is implied behind "Jupiter" identifier and is about what it is similar to.

Will it be usable and useful? Starting with the way of disambiguating "Jupiter {is} planet" from "Jupiter {is} god": it makes sense to specify this by content authors rather than rely on someone or something, which may guess what these authors really meant but may not. Further, you can notice "has/of" relations are very similar to directory levels but semantic identifiers are integral whereas file operations can break semantic links (you can do this by moving a file from "Jupiter/chemistry/reports" to "temp" directory). What is benefit of defining "Galaxy {has} Solar System" and "Solar System {has} Jupiter" rather as "rules" comparing with when they are defined as directories/menus/windows? As "rules" they can be used in inference, for example, if you are looking for "chemistry {has} reports {of} Solar System {has} planets" then "report {of} chemistry {of} Jupiter" will match because "Jupiter {is} planet" and "Solar System {has} Jupiter". Why "chemistry {of} Jupiter" is not "rule" but explicitly present in the identifier? Because "Solar System {has} Jupiter" describes a constant fact whereas a report is rather an ad-hoc combination of meanings. Is it correct to describe "Jupiter {has} chemistry" linked with "has" or it is rather "Jupiter chemistry", that is, an intersection of meanings? In fact, when we say "chemistry" here, we could imply both chemistry as a science (which may only semantically intersect with Jupiter) and a chemical composition (which constitutes Jupiter itself). What we do not consider for sure is a union of "Jupiter" and "chemistry", which could be interpreted as such by search engines in not so good old days. This lead to war of keywords in a pursuit to cover all possible areas of knowledge. But, in fact, it is quite rare when we describe anything by uniting meaning of words. By default, we interpret anything as an intersection, which results in narrower meaning comparing with ones of separate constituents.

Will it be easily reachable/navigable? Yes, as meaning has scope, which is quite similar to directory but is dynamic and can be inferred. For example, scope of "Jupiter" and "Jupiter chemistry" will include "report {of} chemistry {of} Jupiter" directly but "report {of} big planet {of} Solar System" may include it in the result of inference. Of course, not everything can be so easily inferred, for example, "Europa chemistry report". It is not clear if it is about "Europa {is} moon {of} Jupiter" or about "Europe {is} continent {of} Earth", if we spelled correctly then the former, if not then could be the latter. And what none search engine could not distinguish: if "chemistry" is a composition, a science, or an industry. As we see, navigation can be similar to one with directories but in more meaningful way.

Will it be efficient? Semantics should be supported by technics used, say, by search engines. Thus, indexing and relevancy should relate not to "flat" text but to identifiers and relations. For example, "Europa chemistry report" may be included into "Solar System planet chemistry reports" scope, if there are no reports for planets. And this is possible namely because of relevancy between "Europa" and "Solar System planet" through "Jupiter". That is, efficiency can be achieved, in part, due to combining of discrete/deterministic approaches (like semantic networks) and continuous/less deterministic ones (like with search engines).

Will it replace the current user interface? Operations with domain terms could be efficient as they can ignore any differentiation of windows/menus/controls. For example, instead of "Galaxy window -> Solar System table -> Jupiter row -> chemistry button -> report" chain, we can operate only with domain identifiers (e.g. just with "Jupiter chemistry report" one) to reach a target information. Or, instead of thinking in terms of GUI controls and emails like "receive an email, copy-paste information from it, process it and write a report, send an email with own comments", we could think differently: "get information update, merge it with own information, process and change a part of information, send an information update". But, on the other hand, nobody expects natural language will be used for applications which has easily reachable controls on top level (like a calculator). There are a lot of applications, where GUI operations may be blazingly fast comparing with pronouncing instructions or typing semantics.

Will it be reusable? Yes, as soon as we will be able to refer to any meaningful part of applications. For instance, Solar System table has a diameter column, then "Jupiter diameter" value can be exposed and reused outside of the application. This moves us closer to the next topic: application/data interfaces.

<a name="api"></a>Reshaping application/data interfaces
------------------------------------

Any computer application transforms input (data or clicks/touches, etc) into output (data or actions, etc). Unlike with user interface, input and output may be in both human- and machine-friendly form. Very human-hostile binary data can be very effectively processed due to known order of data (eg, "the first ten bytes is a planet name") and by other restrictions and assumptions. However, to integrate input/output with other algorithms or data we need interfaces. If an integration occurs inside the same unit of software (for example, when a library used in an application), efficiency is still can be considered as a priority and integration done by engineers. But heterogeneous environment, lack of resources, avoiding application compatibility issues dictate more flexible (possibly manual) though not so efficient (as for application speed) integration. We cannot avoid it just because we cannot write applications for all possible variants of problems in the world and cannot predict all kinds of problems.

And semantics is namely about integration. Similarity allows us to define if the given interface may deal with given thing. For example, when we want to break a nut, then we need something hard and a stone matches an interface of "hard thing" by similarity. In other words, we try to integrate our goal (to break a nut) with a tool (a stone). Application interfaces are picky as for similarity. If we have Nut class and break() function, which accepts HardThing class/interface as a parameter, then nut.break(stone) will succeed only if a stone object derives from HardThing class/interface (though a mobile phone object may be not derived from it but which can be used as a nut cracker in real life).

Software interfaces (and applications in particular) per se are not meaningful, they hard to use without natural language description. But there is a caveat: many users do not like manuals. Partially because manuals require reader’s attention and understanding, which users may simply lack. Partially because manuals sometimes are not easy to use, navigate, and search. Many think video tutorials may improve this situation but there is another problem: some videos are lengthy too, sometimes require even more time than manual, sometimes they are hard to divide and they are not referable, etc. And there is one more caveat: many software engineers consider any natural language activity as just an addendum to software and do not like it very much. How it can be different, if requirements and documentation are not linked with code explicitly (only through engineer understanding)? Some people try to fight with it through naming conventions, self-documenting code, behavior driven development, etc. But naming conventions are finally only conventions and hardly can be used or analyzed by software. Self-documenting code is quite good convention but you should support synchronization between self-documentation and code because ties between code and natural language are in a developer mind only and can be not so easily grasped even by other developers. Behavior-driven approach allows collaboration through specifications in natural language but their meaning is clear only for tools, which are able to process them. Therefore, finally, we again use natural language to explain meaning. But it is an implicit semantic form unless interpreted by humans. Can we use an explicit one instead?

Imagine we work with planet application, which includes planet diameters but does not planet volumes. Third-party libraries may provide getBallVolume(diameter), fetch_sphere_volume(radius), or globe.volume() function/methods. But how do we know we can apply ball/sphere/globe related functions to planets? Just by presence of radius or diameter property? But circles and sectors have it too. By inheritance/complying with SphericalObject class or BallLike interface? But modern algorithms have no means to conclude that inheritance from SphericalObject means that a Planet class is similar to a sphere. Of course, such conclusions can be made by software engineers but can they be done automatically?

Semantics implies, if we want to have an answer with planet volumes, we need to ask "What{_} {is} volume {of} planet" question (here "is" is rather "is value of"). By "{_}" we imply an unknown, which will be found if the rest of the question (that is, "volume {of} planet") matches something. Similarly, semantics implies given functions/methods correspond to "What{_} {is} volume {of} ball {has} diameter?" for getBallVolume(diameter), "What{_} {is} volume {of} sphere {has} radius?" for fetch_sphere_volume(radius), and "What{_} {is} volume {of} globe?" for globe.volume(). We omit here, (a) a globe object should be constructed with either a radius or a diameter field, (b) a radius is converted to a diameter or vice versa, (c) unit managing (as an output has cubic correspondence of an input). What is more important: could these function interfaces match our question? Yes, because (a) planet is similar to ball, sphere, globe, which can be defined with "planet {is} ball", "ball {is} sphere", "ball {is} globe", (b) "volume" matches directly. After this, what remains is to substitute, say, "ball {has} diameter" with value of "diameter {of} planet" and execute a question (function).

What are advantages of such re-definitions? First, we may operate with objects/fields/functions/parameters more flexibly, for example, when we say an interface is "What{_} {is} volume {of} ball {has} diameter?", it means we can operate with "volume", "ball", and "diameter" as single units (find similarities, dependencies, etc). Second, additional flexibility comes as we do not attach identifiers to software concepts (objects/fields/functions/parameters) with own restrictions, which do not affect natural language identifiers. Third, such interfaces can be handled with pseudo natural language queries and quasi semantics requests, that is, code may become searchable and integrated easier.

Certainly, such process won't be so simple, we already mentioned several factors, which we do not consider above. And it is even more complicated. Realize, we try to apply such interfaces to substring function, which may be described as: "Returns a substring of the string. The substring begins at startIdx (inclusive) and ends at endIdx (exclusive). An error occurs, if startIdx is negative, or endIdx is bigger than length of the string length, or startIdx is bigger than endIdx." An interface may look simple: "What{_} {is} substring {of} string {has #1 #2} start index {#1} end index {#2}". Usual user request may look simple too: "Find me substring of 'Jupiter' from the beginning until the third character". But this means we need to keep in mind: (a) string indexes start from 0 or 1, (b) start/end indexes can be inclusive or exclusive, (c) how to convert ordinal numbers and definitions like "the beginning" to indexes, (d) an interface should return meaningful explanation why a function fails, for example, "failure {of} substring {caused by} start index {is less} end index", etc.

Have you noticed substring function description may serve both as requirements, documentation, and even specifies error handling? What's more, such descriptions may also include architecture traits, testing expectations, user interface description, packaging and deployment instructions, configuration data and dependencies, monitoring indicators, etc. And we can markup such descriptions to make them more meaningful. What can motivate us? Ability to have requirements, which may be linked with other levels, thus making them more closely integrated and verifiable. Ability to have closer integration ties between architecture and implementation and to track changes in both levels. Ability to re-define software (user/application/data) interfaces as described above. Ability to have self-explainable configuration dependencies (for example, what option disables a function). Ability to have applications, which can explain own failures. Ability to outline documentation by requirements, code, and tests, and do it verifiable too. Ability to have more or less seamless continuous integration at wider scale.

<a name="future"></a>Reshaping the Future
--------------------

How all this principally different from other approaches? Imagine worldwide "Web of questions". It is not "Web of Data", it is not about restricted sets of metadata, it is not about metadata at all. These questions can be understood not only by machines but by humans too. Such approach can be quite fine grained, flexible, searchable, be easier integrated, be detailed as much as necessary. Imagine continuous integration but on wider scale. If we have "The application has to provide planet diameter" requirement, it can be marked up as "The application has to provide {} planet {has} diameter", which produces "planet {has} diameter" requirement statement. Further, we can markup: (a) planet class and diameter property or method on architecture class diagram, (b) the same in code, (c) appropriate test expectations and cases, (d) appropriate parts of user interface, (e) corresponding documentation, etc. Thus, we can check if all requirements are covered with architecture, code, tests, user interface, and documentation. And vice versa, we can check what classes/properties, test expectations/tests, needs in user interface controls or documentation were detected, additionally to original requirements. 

Moreover, expected failures should be a part of requirements and documentation too. Applications should fail with meaningful explanations of the cause, applications should explain why functions are disabled, which configuration options affect given area of user interface, etc. For that, code should be constructed not only from the point of view of changing but also from the one of cause-effects. For example, we can link planet-diameter-disabled option by cause-effect relations with enabling of "planet {has} diameter" function. Then, if a user (or a test case) asks why this function is disabled, an application may answer that planet-diameter-disabled option is false. Of course, such answer should be meaningful too, then, for example, if one service has disabled planet diameter function and another has disabled planet mass function, then on the question why planet density is disabled, explanations of both services may be combined.

Apparently, there are a lot of problems need to be resolved and discussed. What's about security and veracity? Which relations are sine qua non and without which we can live? Which relations to use? Which parts of natural language are meaningful and in what way? All these problems require thorough consideration. But meaningful markup allows to start quickly. We can use simple disambiguation: for separating identifiers, for making them as precise as possible, for distinguishing basic relations. We can apply natural language interfaces for already existing code with input/output and data.

For instance, if we have getPlanetDiameter() function, which accepts null-terminated Unicode string and returns 32-bit integer, then a function interface may look like "What {_} {is} diameter {of} planet?" An adapter should link identifiers of the question with input/output and do corresponding conversions. Even simpler example can be a web page with a table or inline text with planet diameters, which marked up to expose this question. Thus, a legacy function can be used as a sort of web page, say, in a search. And a web page can be used as a sort of function, say, in conjunction with other functions in algorithm. In fact, modern search engines already can answer "What is diameter of planet?" question. But. They can answer only to restricted set of questions. And their answers are not reusable by other algorithms. Instead, the proposed approach through markup allows to answer any question, provided by third-party applications or data. And results will be really reusable. And it can be easily [implemented](https://github.com/meaningfuljs/meaningfuljs).

All this won’t make software understand you but at least can allow to get more meaningful answers from it. Software should not be a magic black box, which somehow reacts to our words. Both Natural and Artificial Intelligence should collaborate in the way explicit for both sides. Even human-to-human communication may benefit if thoughts will be expressed more clearly. And all this can change the world, can't?